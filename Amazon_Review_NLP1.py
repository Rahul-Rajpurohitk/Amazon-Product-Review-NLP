# -*- coding: utf-8 -*-
"""Untitled (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10uZsm7NTry_9q94FdfleAR4dYcEZHPuh
"""

import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
import string

import nltk
from nltk.corpus import words

# Download the NLTK words dataset if not already downloaded
nltk.download('words')

# Get the list of words from NLTK's words corpus
word_list = words.words()

# Calculate the total number of words
total_words = len(word_list)

print("Total number of words in NLTK's words corpus:", total_words)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
import re

# Load the dataset
df = pd.read_csv('/content/amazon_reviews.csv')  # Replace 'your_dataset.csv' with the actual dataset file path

df['reviewText'] = df['reviewText'].astype(str)

"""# Testing the preprocess function."""

def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()
    # Remove punctuation and digits while preserving spaces
    text = ''.join([char if char not in string.punctuation and not char.isdigit() else ' ' for char in text])
    # Tokenize the text using a regular expression to capture words
    #tokens = re.findall(r'\b\w+\b', text)
    tokens = nltk.word_tokenize(text)
    # Remove stopwords

    tokens = [word for word in tokens if word not in stopwords.words('english')]
    #Keep the words that are actually english words and remove the noisy words like "aac"
    tokens = [word for word in tokens if word in word_list]
    return ' '.join(tokens)

# Apply text preprocessing to the 'review' column
df['reviewText'] = df['reviewText'].apply(preprocess_text)

"""2. Representation of Texts: word vectors (40 points)

1) Count-based word vectors with co-occurrence matrix
"""

def get_vocab(corpus):
    # Create a list of words from the tokenized corpus
    #all_words = [word for review in corpus for word in review.split()]
    all_words = [word for review in corpus for word in review.split() if word.strip()]
    # Get distinct words by converting the list to a set (removes duplicates) and then back to a sorted list
    corpus_words = sorted(list(set(all_words)))

    return corpus_words

# Apply the get_vocab function to the training data
corpus = df['reviewText']
corpus_words = get_vocab(corpus)

# Print the first 20 words in the sorted vocabulary
print("Vocabulary (first 20 words):", corpus_words[:])

# Print the total number of distinct words in the vocabulary
print("Total number of distinct words:", len(corpus_words))

import numpy as np

def compute_co_occurrence_matrix(corpus_words, corpus, window_size=4):
    word2index = {word: index for index, word in enumerate(corpus_words)}
    vocab_size = len(corpus_words)

    # Initialize the co-occurrence matrix with zeros
    M = np.zeros((vocab_size, vocab_size), dtype=np.int32)

    for review_tokens in corpus:
        for center_index, center_word in enumerate(review_tokens):
            center_word_index = word2index.get(center_word)
            if center_word_index is not None:
                left_boundary = max(0, center_index - window_size)
                right_boundary = min(len(review_tokens), center_index + window_size + 1)

                context_indices = [word2index.get(review_tokens[i]) for i in range(left_boundary, right_boundary) if i != center_index]
                context_indices = [i for i in context_indices if i is not None]

                for context_index in context_indices:
                    M[center_word_index, context_index] += 1

    return M, word2index

# Call the compute_co_occurrence_matrix function
M, word2index = compute_co_occurrence_matrix(corpus_words, corpus, window_size=4)

# Print the co-occurrence matrix and word2index dictionary (for demonstration purposes)
print("Co-occurrence Matrix:")
print(M)
print("\nWord to Index Dictionary:")
print(word2index)

# Assuming M is your co-occurrence matrix
if np.all(M == 0):
    print("The co-occurrence matrix M is empty (contains all zeros).")
else:
    print("The co-occurrence matrix M is not empty.")

import numpy as np

# Assuming M is your co-occurrence matrix
non_zero_values = M[M != 0]

# Now, non_zero_values contains all values from M that are not equal to 0
print(non_zero_values)

from sklearn.decomposition import TruncatedSVD

def reduce_to_k_dim(M):
    # Initialize TruncatedSVD with the desired number of components (k)
    svd = TruncatedSVD(n_iter=10, random_state=42)

    # Fit and transform the co-occurrence matrix M
    M_reduced = svd.fit_transform(M)

    return M_reduced

M_reduced = reduce_to_k_dim(M)

import numpy as np

# Assuming M is your co-occurrence matrix
non_zero_values = M_reduced[M_reduced != 0]

# Now, non_zero_values contains all values from M that are not equal to 0
print(non_zero_values)

import matplotlib.pyplot as plt

def plot_embeddings(M_reduced, word2index, words_to_plot):
    # Get indices of words to plot
    word_indices = [word2index[word] for word in words_to_plot if word in word2index]

    # Extract the reduced embeddings for the specified words
    word_embeddings = M_reduced[word_indices]

    # Create a scatterplot
    plt.figure(figsize=(10, 6))
    plt.scatter(word_embeddings[:, 0], word_embeddings[:, 1], marker='o', color='b', alpha=1.0)

    # Annotate the points with the word labels
    for i, word in enumerate(words_to_plot):
        plt.annotate(word, (word_embeddings[i, 0], word_embeddings[i, 1]))

    # Set plot labels
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.title('Word Embeddings Scatterplot')

    # Show the plot
    plt.show()

# Example usage with the specified words
words_to_plot = ['purchase', 'buy', 'work', 'got', 'ordered', 'received', 'product', 'item', 'deal', 'use']
plot_embeddings(M_reduced, word2index, words_to_plot)

def load_embedding_model():
    """ Load GloVe Vectors
    Return:
    wv_from_bin: All 400000 embeddings, each lengh 200
    """
    import gensim.downloader as api
    wv_from_bin = api.load("glove-wiki-gigaword-200")
    print("Loaded vocab size %i" % len(list(wv_from_bin.index_to_key)))
    return wv_from_bin
    # -----------------------------------
    # Run Cell to Load Word Vectors
    # Note: This will take a couple minutes
    # -----------------------------------
wv_from_bin = load_embedding_model()

def get_matrix_of_vectors(wv_from_bin, corpus_words):
    """ Put the GloVe vectors into a matrix M.
    Param:
    wv_from_bin: KeyedVectors object; the 400000 GloVe vectors loaded from file
    Return:
    M: numpy matrix shape (num words, 200) containing the vectors
    word2ind: dictionary mapping each word to its row number in M
    """
    import random
    words = list(wv_from_bin.index_to_key)
    print("Shuffling words ...")
    random.seed(225)
    random.shuffle(words)
    words = words[:10000]
    print("Putting %i words into word2ind and matrix M..." % len(words))
    word2ind = {}
    M = []
    curInd = 0
    for w in words:
        try:
            M.append(wv_from_bin.get_vector(w))
            word2ind[w] = curInd
            curInd += 1
        except KeyError:
            continue
    for w in corpus_words:
        if w in words:
            continue
        try:
            M.append(wv_from_bin.get_vector(w))
            word2ind[w] = curInd
            curInd += 1
        except KeyError:
            continue
    M = np.stack(M)
    print("Done.")
    return M, word2ind

M2, word2ind = get_matrix_of_vectors(wv_from_bin, corpus_words)

M2_reduced = reduce_to_k_dim(M2)

words_to_plot = ['purchase', 'buy', 'work', 'got', 'ordered', 'received', 'product', 'item', 'deal', 'use']
plot_embeddings(M2_reduced, word2ind, words_to_plot)

from sklearn.decomposition import TruncatedSVD

def reduce_to_k_dim(M):
    # Initialize TruncatedSVD with the desired number of components (k)
    svd = TruncatedSVD(n_components = 128, n_iter=10, random_state=42)

    # Fit and transform the co-occurrence matrix M
    M_reduced = svd.fit_transform(M)

    return M_reduced

M3_reduced = reduce_to_k_dim(M2)

def calculate_review_embedding(tokens, word_embeddings, word2ind):


    # Initialize an array to store the word embeddings for the tokens in the review
    review_word_embeddings = []

    # Iterate through the tokens in the review
    for token in tokens:
        # Check if the token is in the word2ind dictionary
        if token in word2ind:
            # Get the index of the token in the word2ind dictionary
            index = word2ind[token]
            # Get the corresponding word embedding from word_embeddings
            word_embedding = word_embeddings[index]
            # Append the word embedding to the review_word_embeddings array
            review_word_embeddings.append(word_embedding)

    # Calculate the average of the word embeddings for the review
    if len(review_word_embeddings) > 0:
        review_embedding = np.mean(review_word_embeddings, axis=0)
    else:
        review_embedding = np.zeros(word_embeddings.shape[1])  # Use zeros if no valid embeddings found

    return review_embedding

# Initialize an array to store the review embeddings
review_embeddings = []

# Iterate through the reviews in your dataset
for review in corpus:  # Replace with the actual variable name for your reviews
    # Calculate the review embedding for the current review
    review_embedding = calculate_review_embedding(review, M3_reduced, word2ind)
    # Append the review embedding to the review_embeddings array
    review_embeddings.append(review_embedding)

# Convert the review_embeddings array to a numpy array
review_embeddings = np.array(review_embeddings)

review_embeddings.shape

new_df = pd.concat([df, pd.DataFrame(review_embeddings)], axis=1)

new_df.shape

df.shape

new_df.columns = new_df.columns.astype(str)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.neural_network import MLPClassifier

features = new_df.drop(columns=['overall', 'reviewText'])

# Split the data into training, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(features, new_df['overall'], test_size=0.2, random_state=42, stratify=df['overall'])
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# Create and train the logistic regression model
logistic_model = LogisticRegression(penalty='l2', random_state=42, max_iter=1000)
logistic_model.fit(X_train, y_train)

# Predict on the validation set
y_val_pred = logistic_model.predict(X_val)

# Evaluate the model on the validation set
accuracy = accuracy_score(y_val, y_val_pred)
precision = precision_score(y_val, y_val_pred, average='weighted')
recall = recall_score(y_val, y_val_pred, average='weighted')
f1 = f1_score(y_val, y_val_pred, average='weighted')
roc_auc = roc_auc_score(y_val, logistic_model.predict_proba(X_val), multi_class='ovr')

# Print evaluation metrics
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("ROC AUC:", roc_auc)

from sklearn.neural_network import MLPClassifier

# Create and train the neural network model
nn_model = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', random_state=42, max_iter=1000)
nn_model.fit(X_train, y_train)

# Predict on the validation set
y_val_pred = nn_model.predict(X_val)

# Evaluate the model on the validation set
accuracy = accuracy_score(y_val, y_val_pred)
precision = precision_score(y_val, y_val_pred, average='weighted')
recall = recall_score(y_val, y_val_pred, average='weighted')
f1 = f1_score(y_val, y_val_pred, average='weighted')
roc_auc = roc_auc_score(y_val, nn_model.predict_proba(X_val), multi_class='ovr')

# Print evaluation metrics
print("Neural Network Model - Validation Metrics:")
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("ROC AUC:", roc_auc)

"""The Neural-Network has performed well in the sense of ROC but the Logistic had better accuracy, Later when we tested our test data on Logistic it improved results in ROC as well."""

# Predict on the test set
y_test_pred = logistic_model.predict(X_test)

# Evaluate the model on the test set
accuracy = accuracy_score(y_test, y_test_pred)
precision = precision_score(y_test, y_test_pred, average='weighted')
recall = recall_score(y_test, y_test_pred, average='weighted')
f1 = f1_score(y_test, y_test_pred, average='weighted')
roc_auc = roc_auc_score(y_test, logistic_model.predict_proba(X_test), multi_class='ovr')

# Print evaluation metrics
print("Test Metrics for Logistic Regression Model:")
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("ROC AUC:", roc_auc)

